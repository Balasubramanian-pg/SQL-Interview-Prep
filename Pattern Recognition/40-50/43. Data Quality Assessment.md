## **Pattern**: Identifying and quantifying data quality issues.

**Decomposition Strategy**:

1. Define data quality dimensions (completeness, accuracy, etc.)
2. Create metrics for each dimension
3. Set thresholds for acceptable quality
4. Flag problematic data areas

**Example**: "Assess data quality across customer records."

```SQL
WITH completeness AS (
    SELECT
        COUNT(*) AS total_records,
        SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) AS missing_email,
        SUM(CASE WHEN phone IS NULL THEN 1 ELSE 0 END) AS missing_phone,
        SUM(CASE WHEN address IS NULL THEN 1 ELSE 0 END) AS missing_address
    FROM customers
),

format_validity AS (
    SELECT
        SUM(CASE WHEN email NOT LIKE '%@%.%' THEN 1 ELSE 0 END) AS invalid_email_format,
        SUM(CASE WHEN phone NOT REGEXP '^[0-9]{10}$' THEN 1 ELSE 0 END) AS invalid_phone_format,
        SUM(CASE WHEN postal_code NOT REGEXP '^[0-9]{5}(-[0-9]{4})?$' THEN 1 ELSE 0 END) AS invalid_postal_format
    FROM customers
),

duplication AS (
    SELECT
        COUNT(*) - COUNT(DISTINCT email) AS duplicate_emails,
        COUNT(*) - COUNT(DISTINCT phone) AS duplicate_phones
    FROM customers
    WHERE email IS NOT NULL AND phone IS NOT NULL
)

SELECT
    c.total_records,
    c.missing_email * 100.0 / c.total_records AS pct_missing_email,
    c.missing_phone * 100.0 / c.total_records AS pct_missing_phone,
    c.missing_address * 100.0 / c.total_records AS pct_missing_address,
    f.invalid_email_format * 100.0 / (c.total_records - c.missing_email) AS pct_invalid_email,
    f.invalid_phone_format * 100.0 / (c.total_records - c.missing_phone) AS pct_invalid_phone,
    d.duplicate_emails * 100.0 / (c.total_records - c.missing_email) AS pct_duplicate_email,
    d.duplicate_phones * 100.0 / (c.total_records - c.missing_phone) AS pct_duplicate_phone
FROM completeness c
CROSS JOIN format_validity f
CROSS JOIN duplication d;
```
Your example provides a solid framework for assessing data quality using SQL, particularly by focusing on the dimensions of **completeness**, **validity**, and **uniqueness (duplication)**. This is a practical and direct approach to a critical data management task. 

### Analysis of Assumptions

Your SQL code and strategy operate on several key assumptions:

* **Rule-based Validation**: The strategy relies on predefined business rules and regular expressions (e.g., `email NOT LIKE '%@%.%'`, `phone NOT REGEXP '^[0-9]{10}$'`) to check data validity. This is an efficient way to catch common errors, but it assumes that the rules you've defined are sufficient and comprehensive. It won't catch logical errors or semantic issues (e.g., a valid email format for a defunct account or a correct phone number for a non-existent person).
* **Completeness is a simple null check**: You correctly identify missing data with `IS NULL` checks. This is the most common way to measure completeness. However, it assumes that an empty string `''` or a placeholder value like `'N/A'` is not used to represent missing data. These would require additional `CASE` statements to be identified.
* **Duplicates are based on a few key columns**: Your duplication check relies on `email` and `phone` uniqueness. This is reasonable, as these are often used as identifiers. However, it assumes that there are no "fuzzy" duplicates (e.g., `'John Smith'` vs. `'Jon Smith'`) or records with different identifiers but belonging to the same entity. Detecting these requires more advanced techniques like fuzzy matching or probabilistic record linkage.

### Counterpoints & Alternative Perspectives

A data governance professional might challenge the limitations of a purely SQL-based approach for a holistic data quality program.

* **Beyond SQL**: While SQL is excellent for measuring quantifiable data quality, it is less effective for dimensions like **accuracy** (is the data correct in the real world?), **timeliness** (is the data up-to-date?), or **consistency** (is data from different systems in agreement?). For these, you often need to compare data against external sources or a "golden record," a task better suited for dedicated data quality tools or data integration platforms.
* **The Problem of `CROSS JOIN`**: Your final query uses `CROSS JOIN` to combine the results of the three CTEs. This works perfectly because each CTE returns a single row. However, if any CTE were to return multiple rows, a `CROSS JOIN` would produce a Cartesian product, which is likely not the desired result. While it's correct here, it's a practice that requires caution. A more explicit and sometimes safer approach would be to calculate all metrics within a single, more complex CTE if performance allows.
* **Metrics vs. Actions**: The query provides a fantastic **report**, but it doesn't provide a direct list of the problematic rows. The next step in a data quality process would be to run a separate query to `SELECT *` where `email IS NULL` or `phone NOT REGEXP ...` to identify the specific records that need to be cleaned. Your strategy's "Flag problematic data areas" step is essential, and the provided SQL is a perfect tool for creating that high-level flag, but not the detailed report.

### Breaking Down the Code into Components

Your code is a clean and modular example of how to decompose a complex problem into manageable parts using CTEs.

#### **1. Completeness Check (`completeness` CTE)**

* This CTE calculates the **percentage of missing values** for each key field (`email`, `phone`, `address`).
* It uses `SUM(CASE WHEN ... THEN 1 ELSE 0 END)` to count the number of records with `NULL` values. This is a common and highly efficient SQL pattern for conditional counting.
* This metric is foundational to data quality assessment; if you don't have the data, you can't use it.

#### **2. Format Validity Check (`format_validity` CTE)**

* This component measures **conformity to rules**.
* It uses `LIKE` and `REGEXP` (or `SIMILAR TO` in some SQL dialects) to check if data adheres to a specified format. For instance, the `email` check is a basic sanity test, while the `phone` and `postal_code` checks are more specific.
* The logic correctly identifies that a value is invalid only if it exists in the first place, thus it's crucial that it's applied after a completeness check.

#### **3. Uniqueness Check (`duplication` CTE)**

* This component identifies **duplicate records**.
* The logic `COUNT(*) - COUNT(DISTINCT ...)` is an elegant and efficient way to calculate the number of non-unique entries. It works because the difference between the total number of records and the number of unique values must be the number of duplicates (assuming each duplicate pair is counted once).

#### **4. Final Reporting (`SELECT ...`)**

* This final step combines all the metrics and presents them in a single, easy-to-read report.
* It converts the raw counts into **percentages** by dividing by the total record count. This makes the results more comparable and easier to interpret, such as "95% of records have a valid email address." The careful use of `(c.total_records - c.missing_email)` as the denominator for the validity and duplication checks is an important detail, as it correctly calculates the percentage of valid or duplicated records *among those that actually exist*.
