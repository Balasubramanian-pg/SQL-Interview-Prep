### Pattern 1: Aggregation with Multiple Conditions

**Decomposition Strategy:**

1.  **Filter first:** Get rid of junk data you don't need *before* doing anything else. It's just more efficient.
2.  **Create groups:** Lump similar things together using `GROUP BY`.
3.  **Apply aggregations:** Do your `COUNT()`, `SUM()`, `AVG()`, whatever, on those groups.
4.  **Filter aggregated results:** If you only care about groups with a certain count (e.g., `HAVING COUNT(*) > 10`), filter again at the end.

> [!TIP]
> Seriously, filter as early as possible. Why make the database do a bunch of math on rows you're just going to throw away later? It's inefficient. Just do it right the first time.

### Pattern 2: Ranking and Top-N Problems

**Decomposition Strategy:**

1.  **Calculate metric:** Figure out what you're ranking by (e.g., `salary`, `sales_amount`).
2.  **Apply window functions:** This is where `RANK()`, `DENSE_RANK()`, or `ROW_NUMBER()` comes in. Use `PARTITION BY` to restart the ranking for each group.
3.  **Filter by rank:** Wrap it all in a CTE (Common Table Expression) and then select where `rank <= N`.

> [!CAUTION]
> Don't sleep on the difference between `RANK()`, `DENSE_RANK()`, and `ROW_NUMBER()`. If there's a tie for 2nd place, `RANK()` skips to 4th, `DENSE_RANK()` goes to 3rd, and `ROW_NUMBER()` just picks one arbitrarily. Knowing which one to use for ties is critical, otherwise your "top 3" might be wrong.

```sql
-- This template is basically muscle memory for Top-N problems
WITH ranked_data AS (
    SELECT
        *,
        DENSE_RANK() OVER (PARTITION BY group_column ORDER BY metric_column DESC) AS rank_num -- I prefer DENSE_RANK(), fight me.
    FROM your_table
)
SELECT * FROM ranked_data WHERE rank_num <= 3; -- N=3 here
```

### Pattern 3: Gaps and Islands Problems

**Decomposition Strategy:**

1.  **Identify start/end points or groups:** This is the tricky part. You're trying to figure out where a continuous sequence ("island") begins or ends, separated by missing data ("gaps").
2.  **Group consecutive items:** Often involves a clever trick where you create a grouping ID by subtracting a row number from the actual date or ID value. If the value increases steadily, this difference stays constant for consecutive items. It feels a bit extra, but it works.
3.  **Aggregate within groups:** Once you have your island groups, you can find the min/max dates or count items within each streak.

> [!NOTE]
> They call this "Gaps and Islands." Sounds like a vacation vibe, not work. But basically, it's for finding streaks. Like, "how many consecutive days did this user log in?" It's a bit advanced, but a solid pattern to know for tougher problems.

### Pattern 4: Hierarchical Data (Manager-Employee)

**Decomposition Strategy:**

1.  **Start with a base case:** Find the people at the very top (e.g., employees where `manager_id IS NULL`). These are your "anchors."
2.  **Recursively build relationships:** Use a Recursive CTE. The recursive part repeatedly joins the table back to itself, finding everyone who reports to the people found in the previous step. It's like a chain reaction down the org chart.
3.  **Aggregate results:** Once you have the full hierarchy, you can count reports, find levels, etc.

> [!WARNING]
> Recursive CTEs can be powerful, but also kinda sus. If you mess up the join condition or don't have a clear base case, you can create an infinite loop. Always double-check your logic and test on small datasets first. Don't be the person who brings down production with a runaway query.

```sql
-- This RECURSIVE keyword feels like magic, but it's just a loop.
WITH RECURSIVE org_hierarchy AS (
    -- 1. Base Case: The CEO or top-level managers
    SELECT employee_id, name, manager_id, 1 AS hierarchy_level
    FROM employees
    WHERE manager_id IS NULL

    UNION ALL

    -- 2. Recursive Case: Join employees to their managers found in the previous step
    SELECT e.employee_id, e.name, e.manager_id, oh.hierarchy_level + 1
    FROM employees e
    JOIN org_hierarchy oh ON e.manager_id = oh.employee_id
)
SELECT * FROM org_hierarchy;
```

## Performance Considerations in Your Decomposition

Don't just write code that "works." Write code that doesn't make the database cry.

1.  **Join Order:** The order you join tables can matter. The old advice is to join smaller tables first. It's not always true with modern optimizers, but still good to think about.
2.  **Early Filtering:** I said it before, I'll say it again: Use `WHERE` *before* joins if possible. Less data to join = faster query. It's common sense.
3.  **Indexing Awareness:** If you're filtering or joining on a column all the time, it should probably have an index. Knowing this makes you look like you actually know how databases work.
4.  **Avoid Expensive Operations:** `DISTINCT` and `ORDER BY` on huge datasets without a `LIMIT` can be slow as heck. Ask yourself if you *really* need them.

## Communication Tips for SQL Interviews

It's not just about getting the right answer; it's about not looking like a chaotic mess while doing it.

1.  **Restate the Problem:** "So, you want me to find..." It shows you were listening and gives you a second to think.
2.  **Explain Your Process:** Before you type a single character, say *how* you plan to solve it. "First, I'll identify the groups. Second, I'll rank them using a window function. Third, I'll filter for the top N."
3.  **Verbalize Steps:** Talk through your code as you write it. It proves you're making intentional choices.
4.  **Discuss Alternatives:** "I'm using `DENSE_RANK()` here because I want to include ties without skipping ranks. If we wanted exactly N rows, I'd use `ROW_NUMBER()`." This is main character energy right here.
5.  **Mention Optimizations:** "For better performance on a large dataset, we'd want an index on the `department` column."

## Common Pitfalls to Avoid

1.  **Overcomplicating:** If a simple `GROUP BY` works, don't try to flex with a complex window function. Keep it practical.
2.  **Ignoring Edge Cases:** Always ask about `NULL` values. "What should happen if the category is NULL?" Also consider empty sets and duplicates. They love to test this.
3.  **Premature Optimization:** Make sure your logic is correct *first*. Don't try to optimize a query that gives the wrong answer. Correctness > speed.
4.  **Not Testing Mental Models:** Walk through your query logic with a tiny bit of sample data. If your logic breaks on 3 rows, it'll definitely break on 3 million.

> [!IMPORTANT]
> They call it "procedural decomposition," which sounds super formal. But really, it just means "thinking step-by-step." Breaking down a problem shows you're structured and not just guessing. This is a skill that's genuinely useful, not just interview fluff.
